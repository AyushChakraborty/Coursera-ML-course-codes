{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT BASED RECOMMENDER SYSTEM IMPLEMENTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we already have the features x, which is comprised of user features and movie features, so its diff form collaborative\n",
    "#filtering in the sense that the features are already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#has 397 users and 847 of movies here, here since both the numbers are not same, some of the rows from both\n",
    "#user and movie dataset has been repeated so as to make them both have the same value, in this case its 50884 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ratings for all users for all movies are given in a singular col itself, and some of them are repeated\n",
    "#as some of the movies and/or users are also repeated/boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user id', 'rating count', 'rating ave', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Horror', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller\\n']\n",
      "['movie id', 'year', 'ave rating', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Horror', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller\\n']\n",
      "50884\n",
      "[4.  3.5 4.  ... 5.  5.  5. ]\n",
      "(50884, 17)\n",
      "(50884, 17)\n"
     ]
    }
   ],
   "source": [
    "#loading the feature data for items and users\n",
    "\n",
    "#movie features\n",
    "item = np.loadtxt('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_item_train.csv', delimiter=',')\n",
    "\n",
    "#headers of movie features\n",
    "with open('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_item_train_header.txt', 'r') as f:\n",
    "    item_header = f.read().split(',')\n",
    "\n",
    "#user features\n",
    "user = np.loadtxt('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_user_train.csv', delimiter=',')\n",
    "\n",
    "#headers of user features\n",
    "with open('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_user_train_header.txt', 'r') as f:\n",
    "    user_header = f.read().split(',')\n",
    "\n",
    "#targets\n",
    "y = np.loadtxt('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_y_train.csv', delimiter=',')\n",
    "\n",
    "#unique movies\n",
    "item_vecs = np.loadtxt('C:\\\\Users\\\\Lenovo\\\\OneDrive\\\\Desktop\\\\coursera_ml\\\\content_item_vecs.csv', delimiter=',')\n",
    "\n",
    "\n",
    "print(user_header)\n",
    "print(item_header)\n",
    "print(len(item))   #50884 movies present \n",
    "print(y)\n",
    "print(item.shape)\n",
    "print(user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_features = user.shape[1] - 3 #removing userid, rate count, ave rating as they are not that important \n",
    "#in contributing useful data \n",
    "num_item_features = item.shape[1] - 1 #removing movieid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_start = 3  #index of col from which to consider the features for users \n",
    "m_start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50884, 1)\n",
      "[[0.55555556]\n",
      " [0.33333333]\n",
      " [0.55555556]\n",
      " ...\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#now since the data in item_train and user_train have very different scales, like some of them are small number some\n",
    "#are large like years and so on hence we need to scale these features using normal scaling\n",
    "\n",
    "#using StandardScaler for the features \n",
    "scalar_item = StandardScaler()\n",
    "scalar_item.fit(item)\n",
    "item_train_scaled = scalar_item.transform(item)\n",
    "\n",
    "scalar_user = StandardScaler()\n",
    "scalar_user.fit(user)\n",
    "user_train_scaled = scalar_user.transform(user)\n",
    "\n",
    "#using MinMaxScalar for the ratings or the targets, MinMaxScalar scales the numbers in the range -1 to 1, here this is\n",
    "#used as it achieves the same effect. The intent of scalaing the targets was so that predicted ratings for users who have\n",
    "#not rated a movie not be 0, but some other number, in this case the prediction would be w.x + b + mu where mu is the \n",
    "#avg rating of that movie for all the users, here its the similar concept where something will be added but its just\n",
    "#not the mean of the movies\n",
    "\n",
    "scaler_target = MinMaxScaler((-1,1))\n",
    "scaler_target.fit(y.reshape(-1,1))\n",
    "y_train_scaled = scaler_target.transform(y.reshape(-1,1))\n",
    "\n",
    "print(y_train_scaled.shape)\n",
    "print(y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data to training and testing \n",
    "item_train, item_test = train_test_split(item_train_scaled, test_size=0.2, random_state=1, shuffle=True)\n",
    "user_train, user_test = train_test_split(user_train_scaled, test_size=0.2, random_state=1, shuffle=True)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40707, 17)\n",
      "(10177, 17)\n",
      "(40707, 17)\n"
     ]
    }
   ],
   "source": [
    "print(item_train.shape)\n",
    "print(item_test.shape)\n",
    "print(user_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m input_user \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(num_user_features, ))  \u001b[38;5;66;03m#input layer for user_NN, symbolic keras tensor \u001b[39;00m\n\u001b[0;32m     18\u001b[0m vu \u001b[38;5;241m=\u001b[39m user_NN(input_user)   \u001b[38;5;66;03m#keras tensor \u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m vu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39ml2_normalize(vu, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m#making sure that mag of vu is always 1\u001b[39;00m\n\u001b[0;32m     22\u001b[0m input_item \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(num_item_features, ))  \u001b[38;5;66;03m#input layer for item_NN\u001b[39;00m\n\u001b[0;32m     23\u001b[0m vm \u001b[38;5;241m=\u001b[39m item_NN(input_item)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "#building the NN to get vu(user vector) and vm(item vector)\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "user_NN = keras.models.Sequential([\n",
    "    layers.Dense(256, 'relu'),   #this is not the input layer \n",
    "    layers.Dense(128, 'relu'),\n",
    "    layers.Dense(32)    #the vector vu for a single user will be outputted from this \n",
    " ])\n",
    "\n",
    "item_NN = keras.models.Sequential([\n",
    "    layers.Dense(256, 'relu'),\n",
    "    layers.Dense(128, 'relu'),\n",
    "    layers.Dense(32)    #the vector vm for a single movie will be outputted from this \n",
    " ])\n",
    "\n",
    "input_user = layers.Input(shape=(num_user_features, ))  #input layer for user_NN, symbolic keras tensor \n",
    "vu = user_NN(input_user)   #keras tensor \n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)  #making sure that mag of vu is always 1\n",
    "\n",
    "\n",
    "input_item = layers.Input(shape=(num_item_features, ))  #input layer for item_NN\n",
    "vm = item_NN(input_item)\n",
    "# vm = tf.linalg.l2_normalize(vm, axis=1)  #making sure that mag of vu is always 1\n",
    "#vm = keras.layers.UnitNormalization(axis=1)(vm)\n",
    "\n",
    "output = layers.Dot(axes=1)([vu, vm])   #dot product of vu and vm taken in another tf layer\n",
    "\n",
    "model = keras.Model([input_user, input_item], output)   #bunching up all the individial sequences and layers to build \n",
    "#a whole model\n",
    "# A KerasTensor cannot be used as input to a TensorFlow function. \"\n",
    "#      93         \"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\n",
    "#      94         \"used when constructing Keras Functional models \"\n",
    "#      95         \"or Keras Functions. You can only use it as input to a Keras layer \"\n",
    "#      96         \"or a Keras operation (from the namespaces `keras.layers` \"\n",
    "#      97         \"and `keras.operations`). \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost = keras.losses.MeanSquaredError()  #instance of this class\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss=cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40707, 14)\n"
     ]
    }
   ],
   "source": [
    "print(user_train[:, u_start:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 2.2654\n",
      "Epoch 2/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.6069\n",
      "Epoch 3/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.6157\n",
      "Epoch 4/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.6286\n",
      "Epoch 5/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.6204\n",
      "Epoch 6/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.6241\n",
      "Epoch 7/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.6452\n",
      "Epoch 8/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.6124\n",
      "Epoch 9/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.6085\n",
      "Epoch 10/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5996\n",
      "Epoch 11/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.5996\n",
      "Epoch 12/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5961\n",
      "Epoch 13/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5989\n",
      "Epoch 14/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.5943\n",
      "Epoch 15/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.5986\n",
      "Epoch 16/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.5913\n",
      "Epoch 17/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5917\n",
      "Epoch 18/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.6018\n",
      "Epoch 19/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5963\n",
      "Epoch 20/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5942\n",
      "Epoch 21/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5936\n",
      "Epoch 22/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.5938\n",
      "Epoch 23/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5913\n",
      "Epoch 24/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5906\n",
      "Epoch 25/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5901\n",
      "Epoch 26/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.5891\n",
      "Epoch 27/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.5919\n",
      "Epoch 28/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.5946\n",
      "Epoch 29/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.5903\n",
      "Epoch 30/30\n",
      "\u001b[1m1273/1273\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.5897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19e74334200>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_start:], item_train[:, m_start:]], y_train, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new user for this purpose making it the 398th user\n",
    "new_user_id = 5000\n",
    "new_rating_ave = 0.0\n",
    "new_action = 0.0\n",
    "new_adventure = 5.0\n",
    "new_animation = 0.0\n",
    "new_childrens = 0.0\n",
    "new_comedy = 0.0\n",
    "new_crime = 0.0\n",
    "new_documentary = 0.0\n",
    "new_drama = 0.0\n",
    "new_fantasy = 5.0\n",
    "new_horror = 0.0\n",
    "new_mystery = 0.0\n",
    "new_romance = 0.0\n",
    "new_scifi = 0.0\n",
    "new_thriller = 0.0\n",
    "new_rating_count = 3\n",
    "\n",
    "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "                      new_action, new_adventure, new_animation, new_childrens,\n",
    "                      new_comedy, new_crime, new_documentary,\n",
    "                      new_drama, new_fantasy, new_horror, new_mystery,\n",
    "                      new_romance, new_scifi, new_thriller]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "[[-232.43585 ]\n",
      " [-231.43546 ]\n",
      " [-238.01591 ]\n",
      " [-243.55716 ]\n",
      " [-264.1941  ]\n",
      " [-209.7782  ]\n",
      " [-240.11905 ]\n",
      " [-188.16556 ]\n",
      " [-289.98874 ]\n",
      " [-257.41232 ]\n",
      " [-187.82138 ]\n",
      " [-132.50919 ]\n",
      " [-164.46262 ]\n",
      " [-272.08688 ]\n",
      " [-308.7956  ]\n",
      " [-239.12181 ]\n",
      " [-196.12042 ]\n",
      " [-138.63539 ]\n",
      " [-163.43707 ]\n",
      " [-224.16173 ]\n",
      " [-323.55392 ]\n",
      " [-232.88733 ]\n",
      " [-197.39752 ]\n",
      " [-215.01807 ]\n",
      " [-247.73422 ]\n",
      " [-301.71506 ]\n",
      " [-216.2642  ]\n",
      " [-200.32558 ]\n",
      " [-268.12064 ]\n",
      " [-274.5268  ]\n",
      " [-190.41666 ]\n",
      " [-210.89566 ]\n",
      " [-177.08493 ]\n",
      " [-247.4243  ]\n",
      " [-260.37427 ]\n",
      " [-194.86159 ]\n",
      " [-250.93607 ]\n",
      " [-233.6265  ]\n",
      " [-207.68845 ]\n",
      " [-241.91292 ]\n",
      " [ -66.6774  ]\n",
      " [-242.8829  ]\n",
      " [-240.77191 ]\n",
      " [-131.40297 ]\n",
      " [-202.47171 ]\n",
      " [-212.16086 ]\n",
      " [ -88.35124 ]\n",
      " [ -48.464222]\n",
      " [-150.05463 ]\n",
      " [-135.38661 ]\n",
      " [-297.4781  ]\n",
      " [ -79.91008 ]\n",
      " [-139.9889  ]\n",
      " [-152.4882  ]\n",
      " [-254.93518 ]\n",
      " [-211.17027 ]\n",
      " [-170.73819 ]\n",
      " [-159.40707 ]\n",
      " [-166.5516  ]\n",
      " [-232.31339 ]\n",
      " [-203.18181 ]\n",
      " [-137.28638 ]\n",
      " [-112.60663 ]\n",
      " [ -95.950676]\n",
      " [-281.9156  ]\n",
      " [-142.81    ]\n",
      " [-174.85873 ]\n",
      " [-207.03168 ]\n",
      " [-116.26336 ]\n",
      " [-176.55972 ]\n",
      " [-115.89554 ]\n",
      " [-151.44667 ]\n",
      " [-130.34724 ]\n",
      " [-163.20839 ]\n",
      " [-190.78374 ]\n",
      " [-276.29538 ]\n",
      " [-128.19547 ]\n",
      " [-150.55258 ]\n",
      " [-150.64221 ]\n",
      " [-172.38223 ]\n",
      " [-115.37507 ]\n",
      " [-225.67957 ]\n",
      " [-124.492386]\n",
      " [-314.52505 ]\n",
      " [-146.42923 ]\n",
      " [-195.98679 ]\n",
      " [-248.36719 ]\n",
      " [-187.20726 ]\n",
      " [-189.04846 ]\n",
      " [-300.62308 ]\n",
      " [-200.06693 ]\n",
      " [-184.97444 ]\n",
      " [-240.11917 ]\n",
      " [-207.2645  ]\n",
      " [-254.92175 ]\n",
      " [-124.77869 ]\n",
      " [-232.21912 ]\n",
      " [-202.28119 ]\n",
      " [-298.14264 ]\n",
      " [-178.1445  ]\n",
      " [-177.83273 ]\n",
      " [-153.0805  ]\n",
      " [-222.38077 ]\n",
      " [-133.86511 ]\n",
      " [-158.51634 ]\n",
      " [-235.8487  ]\n",
      " [-128.7704  ]\n",
      " [-234.23328 ]\n",
      " [-156.99817 ]\n",
      " [-108.94705 ]\n",
      " [-231.43541 ]\n",
      " [-238.44728 ]\n",
      " [-247.79955 ]\n",
      " [-161.62442 ]\n",
      " [-259.7735  ]\n",
      " [-173.35176 ]\n",
      " [-224.08112 ]\n",
      " [-244.24176 ]\n",
      " [-188.51474 ]\n",
      " [-164.9314  ]\n",
      " [-177.22769 ]\n",
      " [-151.43387 ]\n",
      " [ -94.8325  ]\n",
      " [-218.97548 ]\n",
      " [-289.86954 ]\n",
      " [-160.42163 ]\n",
      " [  10.186026]\n",
      " [-164.64636 ]\n",
      " [-146.34404 ]\n",
      " [  10.186026]\n",
      " [-142.82002 ]\n",
      " [-207.17741 ]\n",
      " [-132.97559 ]\n",
      " [-177.40344 ]\n",
      " [-247.90059 ]\n",
      " [-110.3296  ]\n",
      " [-191.20732 ]\n",
      " [-120.32892 ]\n",
      " [-138.5606  ]\n",
      " [-258.44507 ]\n",
      " [-211.24406 ]\n",
      " [-227.15904 ]\n",
      " [-126.70983 ]\n",
      " [-129.92712 ]\n",
      " [-161.8477  ]\n",
      " [-202.55786 ]\n",
      " [-120.078636]\n",
      " [-138.80775 ]\n",
      " [-151.73856 ]\n",
      " [-123.902954]\n",
      " [-153.49452 ]\n",
      " [ -78.60627 ]\n",
      " [-221.00983 ]\n",
      " [-202.3437  ]\n",
      " [-201.55606 ]\n",
      " [-229.08403 ]\n",
      " [-267.7863  ]\n",
      " [-202.15211 ]\n",
      " [-195.42326 ]\n",
      " [-219.63318 ]\n",
      " [-116.45459 ]\n",
      " [-242.59763 ]\n",
      " [-290.72968 ]\n",
      " [-176.68488 ]\n",
      " [-184.68399 ]\n",
      " [ -43.822166]\n",
      " [  10.186026]\n",
      " [-154.43347 ]\n",
      " [-199.83855 ]\n",
      " [-232.21916 ]\n",
      " [-183.70058 ]\n",
      " [-152.0233  ]\n",
      " [-154.7179  ]\n",
      " [-260.09833 ]\n",
      " [-164.72597 ]\n",
      " [-341.26947 ]\n",
      " [ -53.60024 ]\n",
      " [-291.16064 ]\n",
      " [-275.56723 ]\n",
      " [-297.44037 ]\n",
      " [-232.33928 ]\n",
      " [-148.16498 ]\n",
      " [-258.48267 ]\n",
      " [-236.80511 ]\n",
      " [-246.88034 ]\n",
      " [-148.42732 ]\n",
      " [-259.8178  ]\n",
      " [-170.46254 ]\n",
      " [-239.54883 ]\n",
      " [-221.69363 ]\n",
      " [-205.40309 ]\n",
      " [-161.259   ]\n",
      " [-155.11374 ]\n",
      " [-139.31467 ]\n",
      " [-206.61768 ]\n",
      " [-106.733246]\n",
      " [-204.57198 ]\n",
      " [-151.72382 ]\n",
      " [-166.95087 ]\n",
      " [-234.60567 ]\n",
      " [-153.54012 ]\n",
      " [-212.1422  ]\n",
      " [-186.34933 ]\n",
      " [-103.08361 ]\n",
      " [-209.43591 ]\n",
      " [-118.73266 ]\n",
      " [-191.35611 ]\n",
      " [-315.8871  ]\n",
      " [-217.66908 ]\n",
      " [-178.69676 ]\n",
      " [-142.60457 ]\n",
      " [-184.72841 ]\n",
      " [-157.51315 ]\n",
      " [-190.78197 ]\n",
      " [-305.8107  ]\n",
      " [-141.20786 ]\n",
      " [-171.66017 ]\n",
      " [-143.56905 ]\n",
      " [-128.41005 ]\n",
      " [-161.90417 ]\n",
      " [-263.33157 ]\n",
      " [-184.40784 ]\n",
      " [-101.16055 ]\n",
      " [-236.38284 ]\n",
      " [  10.186026]\n",
      " [-160.72168 ]\n",
      " [-192.04617 ]\n",
      " [-239.19347 ]\n",
      " [-220.0092  ]\n",
      " [-164.09775 ]\n",
      " [-202.2632  ]\n",
      " [-166.71176 ]\n",
      " [-179.24799 ]\n",
      " [-243.2449  ]\n",
      " [-158.9462  ]\n",
      " [-212.62636 ]\n",
      " [-192.80264 ]\n",
      " [-193.79544 ]\n",
      " [ -78.283966]\n",
      " [ -63.476654]\n",
      " [-269.44818 ]\n",
      " [ -93.01531 ]\n",
      " [-167.44803 ]\n",
      " [-142.98448 ]\n",
      " [-168.98373 ]\n",
      " [-198.29591 ]\n",
      " [-114.87037 ]\n",
      " [-175.84222 ]\n",
      " [-272.25485 ]\n",
      " [-195.22238 ]\n",
      " [-104.86118 ]\n",
      " [-182.1204  ]\n",
      " [-219.43115 ]\n",
      " [-147.28195 ]\n",
      " [-140.09969 ]\n",
      " [-102.337105]\n",
      " [-200.52806 ]\n",
      " [-299.7981  ]\n",
      " [-219.07031 ]\n",
      " [  10.186026]\n",
      " [-198.7001  ]\n",
      " [-193.65836 ]\n",
      " [-166.06041 ]\n",
      " [  10.186026]\n",
      " [-146.5431  ]\n",
      " [-151.73874 ]\n",
      " [-235.35907 ]\n",
      " [-173.80174 ]\n",
      " [-184.00967 ]\n",
      " [-138.12036 ]\n",
      " [-235.99913 ]\n",
      " [-196.5676  ]\n",
      " [-154.19952 ]\n",
      " [-128.25287 ]\n",
      " [-211.9172  ]\n",
      " [-228.34932 ]\n",
      " [-212.46838 ]\n",
      " [-268.56268 ]\n",
      " [-163.99203 ]\n",
      " [ -67.42281 ]\n",
      " [-309.77042 ]\n",
      " [-207.57687 ]\n",
      " [-150.7782  ]\n",
      " [-185.1889  ]\n",
      " [-281.4493  ]\n",
      " [-148.99477 ]\n",
      " [ -98.41176 ]\n",
      " [-149.43745 ]\n",
      " [-171.5177  ]\n",
      " [-160.61551 ]\n",
      " [-232.40768 ]\n",
      " [-218.14479 ]\n",
      " [-179.15883 ]\n",
      " [-187.31985 ]\n",
      " [-311.63266 ]\n",
      " [-149.71127 ]\n",
      " [-168.20859 ]\n",
      " [-192.26024 ]\n",
      " [-257.90903 ]\n",
      " [-147.82054 ]\n",
      " [-134.8171  ]\n",
      " [-232.917   ]\n",
      " [ -96.879196]\n",
      " [-140.86748 ]\n",
      " [-189.54788 ]\n",
      " [-149.28503 ]\n",
      " [-129.33823 ]\n",
      " [  10.186026]\n",
      " [-107.01006 ]\n",
      " [-155.37262 ]\n",
      " [-160.61478 ]\n",
      " [-246.81403 ]\n",
      " [-190.81885 ]\n",
      " [-186.7374  ]\n",
      " [-203.47768 ]\n",
      " [-199.5031  ]\n",
      " [-238.46786 ]\n",
      " [-307.76712 ]\n",
      " [-152.5887  ]\n",
      " [-212.44386 ]\n",
      " [ -90.37427 ]\n",
      " [-210.08467 ]\n",
      " [-161.76837 ]\n",
      " [-188.93633 ]\n",
      " [-206.49788 ]\n",
      " [-238.13129 ]\n",
      " [ -98.80957 ]\n",
      " [-190.32066 ]\n",
      " [-170.3623  ]\n",
      " [-124.23345 ]\n",
      " [-196.18022 ]\n",
      " [-228.1582  ]\n",
      " [-102.07249 ]\n",
      " [-205.5094  ]\n",
      " [-131.52573 ]\n",
      " [-280.1908  ]\n",
      " [-217.83621 ]\n",
      " [  10.186026]\n",
      " [-257.32843 ]\n",
      " [-183.12973 ]\n",
      " [-195.50056 ]\n",
      " [-129.55402 ]\n",
      " [-197.29407 ]\n",
      " [-333.47742 ]\n",
      " [-179.7646  ]\n",
      " [-275.76447 ]\n",
      " [-189.57635 ]\n",
      " [-200.20348 ]\n",
      " [-133.47205 ]\n",
      " [-158.81357 ]\n",
      " [-314.47266 ]\n",
      " [-224.01532 ]\n",
      " [-253.60864 ]\n",
      " [-242.59377 ]\n",
      " [-162.05798 ]\n",
      " [-157.35704 ]\n",
      " [-126.9085  ]\n",
      " [-158.92807 ]\n",
      " [-194.26772 ]\n",
      " [ -60.3805  ]\n",
      " [-166.71681 ]\n",
      " [-136.18825 ]\n",
      " [-256.96368 ]\n",
      " [-164.89275 ]\n",
      " [-183.48354 ]\n",
      " [-167.02478 ]\n",
      " [-126.20327 ]\n",
      " [-151.0334  ]\n",
      " [-145.55807 ]\n",
      " [-204.96867 ]\n",
      " [-153.61182 ]\n",
      " [-198.33998 ]\n",
      " [-179.20883 ]\n",
      " [-123.231865]\n",
      " [-302.4498  ]\n",
      " [-247.89395 ]\n",
      " [-219.74307 ]\n",
      " [-327.50137 ]\n",
      " [-244.93446 ]\n",
      " [-137.21791 ]\n",
      " [-129.41309 ]\n",
      " [-130.843   ]\n",
      " [ -99.25242 ]\n",
      " [ -83.44431 ]\n",
      " [ -80.90088 ]\n",
      " [-265.4341  ]\n",
      " [-318.67102 ]\n",
      " [-186.21478 ]\n",
      " [-239.5343  ]\n",
      " [-184.47849 ]\n",
      " [-221.01523 ]\n",
      " [-203.18047 ]\n",
      " [-195.408   ]\n",
      " [-256.03577 ]\n",
      " [-220.53528 ]\n",
      " [-292.2281  ]\n",
      " [-196.2284  ]\n",
      " [-217.90816 ]\n",
      " [-188.8533  ]\n",
      " [-178.2428  ]\n",
      " [-158.9096  ]\n",
      " [-201.18385 ]\n",
      " [  10.186026]\n",
      " [-301.5168  ]\n",
      " [-229.20743 ]\n",
      " [-147.27104 ]\n",
      " [-186.21483 ]\n",
      " [-272.07425 ]\n",
      " [-218.53171 ]\n",
      " [-202.90002 ]\n",
      " [-147.56242 ]\n",
      " [-238.44276 ]\n",
      " [-160.44067 ]\n",
      " [-195.0013  ]\n",
      " [-253.41989 ]\n",
      " [ -99.310555]\n",
      " [-166.71213 ]\n",
      " [-150.09688 ]\n",
      " [-159.00502 ]\n",
      " [-123.386765]\n",
      " [-184.45538 ]\n",
      " [-217.1112  ]\n",
      " [-142.76147 ]\n",
      " [ -73.62306 ]\n",
      " [-140.07387 ]\n",
      " [-118.9551  ]\n",
      " [ -76.78977 ]\n",
      " [-193.288   ]\n",
      " [-185.94638 ]\n",
      " [-144.5214  ]\n",
      " [-288.1591  ]\n",
      " [-200.96135 ]\n",
      " [-206.8407  ]\n",
      " [-129.09514 ]\n",
      " [-289.92902 ]\n",
      " [-197.01408 ]\n",
      " [-169.28188 ]\n",
      " [-201.08795 ]\n",
      " [-157.15015 ]\n",
      " [-299.65317 ]\n",
      " [-215.2748  ]\n",
      " [-222.37173 ]\n",
      " [ -63.40791 ]\n",
      " [-247.98409 ]\n",
      " [-139.73605 ]\n",
      " [-154.4366  ]\n",
      " [-143.76347 ]\n",
      " [-216.1907  ]\n",
      " [-102.05802 ]\n",
      " [-180.48776 ]\n",
      " [-159.08945 ]\n",
      " [-218.74205 ]\n",
      " [-112.82388 ]\n",
      " [-232.46027 ]\n",
      " [-161.15878 ]\n",
      " [-194.8622  ]\n",
      " [-108.71455 ]\n",
      " [-251.65735 ]\n",
      " [-190.98622 ]\n",
      " [-175.14154 ]\n",
      " [-274.27618 ]\n",
      " [-167.67468 ]\n",
      " [  10.186026]\n",
      " [-189.96815 ]\n",
      " [-185.80136 ]\n",
      " [-272.96368 ]\n",
      " [-196.36552 ]\n",
      " [-140.5581  ]\n",
      " [-187.45491 ]\n",
      " [-101.02725 ]\n",
      " [-159.96208 ]\n",
      " [-173.64555 ]\n",
      " [-147.40768 ]\n",
      " [-161.6919  ]\n",
      " [  10.186026]\n",
      " [ -80.59626 ]\n",
      " [ -94.95211 ]\n",
      " [-173.7676  ]\n",
      " [-168.63895 ]\n",
      " [-103.49972 ]\n",
      " [-209.22955 ]\n",
      " [-143.67432 ]\n",
      " [-208.69403 ]\n",
      " [-146.9411  ]\n",
      " [-200.90913 ]\n",
      " [-139.9257  ]\n",
      " [ -96.63415 ]\n",
      " [ -93.29324 ]\n",
      " [ -60.377846]\n",
      " [-119.44574 ]\n",
      " [-226.79935 ]\n",
      " [-123.12988 ]\n",
      " [-212.27837 ]\n",
      " [-133.34021 ]\n",
      " [-165.02191 ]\n",
      " [-227.53217 ]\n",
      " [-128.13504 ]\n",
      " [-113.38084 ]\n",
      " [-145.26945 ]\n",
      " [-222.0579  ]\n",
      " [-106.492   ]\n",
      " [-234.43982 ]\n",
      " [-119.292564]\n",
      " [-212.99368 ]\n",
      " [-199.65257 ]\n",
      " [-177.3431  ]\n",
      " [ -45.6954  ]\n",
      " [-131.36385 ]\n",
      " [ -80.36985 ]\n",
      " [-223.84784 ]\n",
      " [-183.55779 ]\n",
      " [-255.62233 ]\n",
      " [-250.30058 ]\n",
      " [-156.37277 ]\n",
      " [ -71.18648 ]\n",
      " [-123.92879 ]\n",
      " [-155.46056 ]\n",
      " [-228.0792  ]\n",
      " [-235.80713 ]\n",
      " [-143.95831 ]\n",
      " [-144.07336 ]\n",
      " [-128.95963 ]\n",
      " [-189.81166 ]\n",
      " [-253.58723 ]\n",
      " [-277.74805 ]\n",
      " [-206.58432 ]\n",
      " [-258.19202 ]\n",
      " [-216.37138 ]\n",
      " [ -93.49251 ]\n",
      " [-220.98308 ]\n",
      " [-216.01796 ]\n",
      " [-198.85909 ]\n",
      " [-226.67719 ]\n",
      " [ -74.82947 ]\n",
      " [-198.49477 ]\n",
      " [-212.77165 ]\n",
      " [-150.0616  ]\n",
      " [-200.23988 ]\n",
      " [ -48.046844]\n",
      " [-168.03143 ]\n",
      " [-190.81429 ]\n",
      " [-152.82999 ]\n",
      " [-140.11769 ]\n",
      " [-179.66273 ]\n",
      " [-115.36177 ]\n",
      " [-197.29042 ]\n",
      " [-191.84886 ]\n",
      " [-220.08008 ]\n",
      " [-185.2326  ]\n",
      " [-262.70175 ]\n",
      " [-116.94394 ]\n",
      " [-120.17064 ]\n",
      " [-166.24512 ]\n",
      " [-123.50256 ]\n",
      " [-192.6045  ]\n",
      " [-194.26497 ]\n",
      " [-228.81976 ]\n",
      " [-229.79736 ]\n",
      " [-140.57782 ]\n",
      " [-137.92226 ]\n",
      " [-158.26495 ]\n",
      " [-235.71835 ]\n",
      " [-126.1735  ]\n",
      " [-200.66312 ]\n",
      " [ -74.91069 ]\n",
      " [-211.7681  ]\n",
      " [-184.73293 ]\n",
      " [-189.75497 ]\n",
      " [-113.75991 ]\n",
      " [ -81.59718 ]\n",
      " [-243.70694 ]\n",
      " [-138.96259 ]\n",
      " [-167.17693 ]\n",
      " [-209.03828 ]\n",
      " [-231.6389  ]\n",
      " [-139.79176 ]\n",
      " [-162.23868 ]\n",
      " [-183.83072 ]\n",
      " [-121.9162  ]\n",
      " [-289.11533 ]\n",
      " [-282.23355 ]\n",
      " [-159.75023 ]\n",
      " [-189.27975 ]\n",
      " [ -84.09444 ]\n",
      " [-144.9334  ]\n",
      " [-159.75023 ]\n",
      " [-118.550316]\n",
      " [-223.4667  ]\n",
      " [-153.06818 ]\n",
      " [-191.19928 ]\n",
      " [-222.40883 ]\n",
      " [-227.08801 ]\n",
      " [ -85.044266]\n",
      " [-260.6617  ]\n",
      " [ -85.101685]\n",
      " [-149.71188 ]\n",
      " [-104.912994]\n",
      " [-258.2911  ]\n",
      " [-172.21098 ]\n",
      " [-160.17696 ]\n",
      " [-172.59166 ]\n",
      " [ -99.32    ]\n",
      " [-207.9082  ]\n",
      " [ -93.3868  ]\n",
      " [-241.44072 ]\n",
      " [-209.04684 ]\n",
      " [-148.21324 ]\n",
      " [-282.46765 ]\n",
      " [-182.35353 ]\n",
      " [-198.46086 ]\n",
      " [  10.186026]\n",
      " [-181.64502 ]\n",
      " [-222.89476 ]\n",
      " [-198.81535 ]\n",
      " [-229.0781  ]\n",
      " [-121.423065]\n",
      " [-264.50967 ]\n",
      " [-200.96176 ]\n",
      " [-158.93295 ]\n",
      " [ -27.363064]\n",
      " [-207.79071 ]\n",
      " [-200.73024 ]\n",
      " [-239.7439  ]\n",
      " [-217.48581 ]\n",
      " [-235.23003 ]\n",
      " [-146.16518 ]\n",
      " [-202.79657 ]\n",
      " [-151.52655 ]\n",
      " [-112.96563 ]\n",
      " [-151.64868 ]\n",
      " [  10.186026]\n",
      " [-196.03952 ]\n",
      " [-239.00017 ]\n",
      " [-156.6507  ]\n",
      " [-176.71495 ]\n",
      " [-174.32855 ]\n",
      " [-124.01993 ]\n",
      " [-132.6958  ]\n",
      " [-136.2532  ]\n",
      " [-163.60439 ]\n",
      " [-215.41377 ]\n",
      " [ -80.13219 ]\n",
      " [-210.82246 ]\n",
      " [-133.71674 ]\n",
      " [-229.7561  ]\n",
      " [-150.9557  ]\n",
      " [-217.91106 ]\n",
      " [-189.55423 ]\n",
      " [-188.32994 ]\n",
      " [-130.63475 ]\n",
      " [-203.94823 ]\n",
      " [-187.39946 ]\n",
      " [-172.02171 ]\n",
      " [-206.4987  ]\n",
      " [-175.1215  ]\n",
      " [-202.83458 ]\n",
      " [-164.95683 ]\n",
      " [-224.24557 ]\n",
      " [-227.36    ]\n",
      " [-145.46861 ]\n",
      " [-144.96553 ]\n",
      " [-296.67722 ]\n",
      " [-295.3071  ]\n",
      " [-183.45827 ]\n",
      " [ -85.45245 ]\n",
      " [-134.11905 ]\n",
      " [-177.09503 ]\n",
      " [-114.44757 ]\n",
      " [-219.28574 ]\n",
      " [-177.48575 ]\n",
      " [-259.47678 ]\n",
      " [-156.60664 ]\n",
      " [-163.29024 ]\n",
      " [-151.65736 ]\n",
      " [-138.31796 ]\n",
      " [-167.07365 ]\n",
      " [-181.42195 ]\n",
      " [-104.93245 ]\n",
      " [-180.17752 ]\n",
      " [-149.08188 ]\n",
      " [-145.67883 ]\n",
      " [-133.15009 ]\n",
      " [-286.48495 ]\n",
      " [-194.72884 ]\n",
      " [-183.09908 ]\n",
      " [-113.80939 ]\n",
      " [-220.70164 ]\n",
      " [ -98.34856 ]\n",
      " [-177.70059 ]\n",
      " [-187.25533 ]\n",
      " [-118.58842 ]\n",
      " [-103.351105]\n",
      " [-261.4495  ]\n",
      " [-228.24667 ]\n",
      " [-125.99363 ]\n",
      " [ -32.76108 ]\n",
      " [-198.02338 ]\n",
      " [-209.66132 ]\n",
      " [-156.38698 ]\n",
      " [-144.33649 ]\n",
      " [-207.64082 ]\n",
      " [-205.67905 ]\n",
      " [-214.25684 ]\n",
      " [-160.59981 ]\n",
      " [-191.84921 ]\n",
      " [-151.73483 ]\n",
      " [-161.86649 ]\n",
      " [-137.56345 ]\n",
      " [ -98.19624 ]\n",
      " [-217.45418 ]\n",
      " [-139.14705 ]\n",
      " [-180.50774 ]\n",
      " [-158.52019 ]\n",
      " [-155.96646 ]\n",
      " [-170.45888 ]\n",
      " [-185.59218 ]\n",
      " [-168.87157 ]\n",
      " [-152.74028 ]\n",
      " [-115.550995]\n",
      " [-107.006615]\n",
      " [-178.23877 ]\n",
      " [-138.54486 ]\n",
      " [-209.8602  ]\n",
      " [-221.66402 ]\n",
      " [-175.79063 ]\n",
      " [-200.11996 ]\n",
      " [-161.43794 ]\n",
      " [-198.81549 ]\n",
      " [-164.19121 ]\n",
      " [-196.67775 ]\n",
      " [-137.45795 ]\n",
      " [-172.04239 ]\n",
      " [-212.43576 ]\n",
      " [-194.58624 ]\n",
      " [-200.60152 ]\n",
      " [-196.29794 ]\n",
      " [-188.56433 ]\n",
      " [-148.34431 ]\n",
      " [-174.15115 ]\n",
      " [-117.37197 ]\n",
      " [-176.42715 ]\n",
      " [ -74.58349 ]\n",
      " [-150.66005 ]\n",
      " [-129.02774 ]\n",
      " [-177.56366 ]\n",
      " [-195.09367 ]\n",
      " [-195.78976 ]\n",
      " [-142.33487 ]\n",
      " [-168.30942 ]\n",
      " [-178.99747 ]\n",
      " [-146.08    ]\n",
      " [-113.72624 ]\n",
      " [-188.49838 ]\n",
      " [-131.38933 ]\n",
      " [ -99.51169 ]\n",
      " [-215.54228 ]\n",
      " [-183.03839 ]\n",
      " [-207.37613 ]\n",
      " [-145.76129 ]\n",
      " [-162.31361 ]\n",
      " [-132.98946 ]\n",
      " [-154.28333 ]\n",
      " [-266.37448 ]\n",
      " [-222.00183 ]\n",
      " [-247.57997 ]\n",
      " [-142.85062 ]\n",
      " [-270.66525 ]\n",
      " [-269.19922 ]\n",
      " [-251.12143 ]\n",
      " [-128.31982 ]\n",
      " [-178.87022 ]\n",
      " [-149.19345 ]\n",
      " [-150.57053 ]\n",
      " [-210.47583 ]\n",
      " [-157.55014 ]\n",
      " [-133.1832  ]\n",
      " [-165.67766 ]\n",
      " [-194.60683 ]\n",
      " [-116.138626]\n",
      " [-224.32478 ]\n",
      " [-160.61845 ]\n",
      " [ -97.555954]\n",
      " [-155.22247 ]\n",
      " [-155.37256 ]\n",
      " [ -85.02028 ]\n",
      " [-135.25523 ]\n",
      " [-130.97835 ]\n",
      " [-134.22728 ]\n",
      " [-138.47041 ]\n",
      " [ -99.043236]\n",
      " [-151.10915 ]\n",
      " [-200.70819 ]\n",
      " [-161.58432 ]\n",
      " [-193.54861 ]\n",
      " [-199.79967 ]\n",
      " [-108.39152 ]\n",
      " [-212.31015 ]\n",
      " [-141.6986  ]\n",
      " [-125.33091 ]\n",
      " [-184.08537 ]\n",
      " [-156.74947 ]\n",
      " [-140.90979 ]\n",
      " [-156.11807 ]\n",
      " [-122.31475 ]\n",
      " [-119.28953 ]\n",
      " [-131.25182 ]\n",
      " [-170.97076 ]\n",
      " [-158.94841 ]\n",
      " [-219.5824  ]\n",
      " [-103.18117 ]\n",
      " [-176.55899 ]\n",
      " [-119.710526]\n",
      " [-148.77249 ]\n",
      " [-207.4892  ]\n",
      " [-148.80486 ]\n",
      " [-163.47504 ]\n",
      " [-162.24147 ]\n",
      " [-223.90002 ]\n",
      " [-191.1312  ]\n",
      " [-249.55925 ]\n",
      " [-218.78821 ]\n",
      " [-290.6963  ]\n",
      " [-168.87343 ]\n",
      " [-203.70341 ]\n",
      " [-149.96234 ]\n",
      " [-126.41792 ]\n",
      " [-188.88501 ]\n",
      " [-127.307465]\n",
      " [-195.95035 ]\n",
      " [-143.53719 ]\n",
      " [ -84.42057 ]\n",
      " [-175.56775 ]\n",
      " [-153.46121 ]\n",
      " [-176.41905 ]\n",
      " [-104.82882 ]\n",
      " [-181.20892 ]\n",
      " [-134.46089 ]\n",
      " [-210.02943 ]\n",
      " [-116.34684 ]\n",
      " [-159.81412 ]\n",
      " [-133.05273 ]\n",
      " [ -89.345024]\n",
      " [-188.88666 ]\n",
      " [-155.88828 ]\n",
      " [-208.3189  ]\n",
      " [-214.09468 ]\n",
      " [-138.18839 ]]\n"
     ]
    }
   ],
   "source": [
    "user_vecs = np.tile(user_vec, (len(item_vecs), 1))   #creating multiple rows of the same user vector for \n",
    "#847 unique movies, this is done as we will pass one row of both the user_vecs and item_vecs to the model one \n",
    "#by one for inference, and this will give us the predictes ratings for all the movies for this new user\n",
    "\n",
    "\n",
    "#scaling the user data using StandardScaler\n",
    "user_vecs = scalar_user.transform(user_vecs)\n",
    "\n",
    "#scaling the movie data using StandardScaler\n",
    "item_vecs = scalar_item.transform(item_vecs)\n",
    "\n",
    "y_pred = model.predict([user_vecs[:, u_start:], item_vecs[:, m_start:]])\n",
    "\n",
    "y_pred = scaler_target.inverse_transform(y_pred)\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
